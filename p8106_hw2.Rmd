---
title: "Homework 2"
author: "Yuki Joyama"
date: "2024-03-11"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message=FALSE,
  warning=FALSE
  )
```

```{r library}
# load libraries
library(tidyverse)
library(rsample) # split data
library(caret)
library(splines)
library(mgcv)
library(earth)
library(ggplot2)
library(vip)
```

```{r dataprep}
# read csv files 
df = read_csv("./College.csv") |> 
  janitor::clean_names() |> 
  dplyr::select(-college) |> 
  dplyr::select(outstate, everything())

# partition (training:test=80:20)
set.seed(100)
data_split = initial_split(df, prop = .80)
train = training(data_split)
test = testing(data_split)
```

The college data is split into train (80%) and test (20%).

# (a) Smoothing Spline
```{r}
# Function to fit smoothing spline model and return predicted values
fit_spline_model <- function(df, df_value) {
  fit.ss <- smooth.spline(df$perc_alumni, y = df$outstate, df = df_value)
  pred.ss <- predict(fit.ss, x = df$perc_alumni)
  return(data.frame(pred = pred.ss$y, perc = df$perc_alumni))
}

# Function to plot smoothed lines with different colors
plot_smooth_lines <- function(train, df_values, colors) {
  p <- ggplot(data = train, aes(x = perc_alumni, y = outstate)) +
    geom_point(color = rgb(.2, .4, .2, .5))
  
  for (i in seq_along(df_values)) {
    df_value <- df_values[i]
    color <- colors[i]
    
    pred.ss.df <- fit_spline_model(train, df_value)
    
    p <- p + geom_line(aes(x = perc, y = pred), data = pred.ss.df, color = color)
  }
  
  p <- p + theme_bw() 
  return(p)
}

# Set range of dfs 
df_values <- c(seq(2, 30, by = 2))
colors <- rainbow(length(df_values))

# Plot smoothed lines
plot_smooth_lines(train, df_values, colors)
```

I set the range of degree of freedom (df) from 2 to 30 by 2 (2, 4, 6, ..., 28, 30). The plot shows that as df increases, the fitted lines become more wiggly.   

To find the optimal df for the model, I will use Generalized cross-validation.  

```{r}
# refit the model using GCV
fit.ss <- smooth.spline(train$perc_alumni, y = train$outstate, cv = FALSE) # determine tuning parameter by min GCV

pred.ss <- predict(
  fit.ss,
  x = train$perc_alumni
)

pred.ss.df <- data.frame(
  pred = pred.ss$y,
  perc = train$perc_alumni
)

# plot
p <- ggplot(
  data = train,
  aes(x = perc_alumni, y = outstate)
) +
  geom_point(color = rgb(.2, .4, .2, .5))

p + geom_line(
  aes(x = perc, y = pred), 
  data = pred.ss.df,
  color = "red"
) + theme_bw()

```

The selected df was `r round(fit.ss$df, 2)` and the plot of this optimal fit is shown above. 

# (b) Multivariate Adaptive Regression Splines (MARS)
```{r cv}
# set up 10-fold cross validation 
ctrl <- trainControl(
  method = "cv",
  number = 10
)
```

```{r}
set.seed(100)

# fit mars model
model.mars <- train(
  x = train[2:17],
  y = train$outstate,
  method = "earth",
  tuneGrid = expand.grid(degree = 1:5, nprune = 2:30),
  metric = "RMSE",
  trControl = ctrl
)

summary(model.mars$finalModel)
coef(model.mars$finalModel)

# best tuning parameters
model.mars$bestTune

plot(model.mars)

# relative variable importance
vip(model.mars$finalModel, type = "nsubsets")
```

The final model can be expressed as the following:  
$\hat{y}$ = 16357.0585 + 1.2722 $\times$ h(2095 - apps) + 0.4506 $\times$ h(apps - 2095) - 3.3926 $\times$ h(1673 - accept)   
+ 0.5027 $\times$ h(accept - 1673) + 3.2937 $\times$ h(903 - enroll) - 1.6789 $\times$ h(1251 - f_undergrad)   
- 0.7292 $\times$ h(f_undergrad - 1251) - 0.9345 $\times$ h(4980 - room_board) + 118.0087 $\times$ h(ph_d - 81)   
- 424.6454 $\times$ h(8.3 - s_f_ratio) - 53.5431 $\times$ h(27 - perc_alumni) - 0.6097 $\times$ h(14820 - expend)  
- 21.6454 $\times$ h(98 - grad_rate) - 219.9621 $\times$ h(grad_rate - 98)  
where $h(.)$ is hinge function.

```{r}
# partial dependence plot of room_board, 
p1 <- pdp::partial(model.mars, pred.var = c("perc_alumni"), grid.resolution = 10) |> 
  autoplot()
p2 <- pdp::partial(model.mars, pred.var = c("room_board", "accept"), grid.resolution = 10) |> 
  pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, screen = list(z = 20, x = -60))

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

The above plot shows the partial dependence plot of `perc_alumni`, `room_board`, and `accept`.

```{r}
# Obtain the test error
mars.pred <- predict(model.mars, newdata = test)
mean((mars.pred - pull(test, "outstate"))^2) # test error
```

The test error is $4.02\times10^6$

# (c) Generalized Additive Model (GAM)
```{r gam}
set.seed(100)
model.gam <- train(x = train[2:17],
                   y = train$outstate,
                   method = "gam",
                   metric = "RMSE",
                   trControl = ctrl)

# check the best tune
model.gam$bestTune

# check final model
summary(model.gam$finalModel)
```

The model includes all the predictors.   
Given the estimated degrees of freedom, I will select nonlinear terms and plot each of them. 

```{r}
# plot
plot(model.gam$finalModel, select = 1)
plot(model.gam$finalModel, select = 3)
plot(model.gam$finalModel, select = 4)
plot(model.gam$finalModel, select = 5)
plot(model.gam$finalModel, select = 6)
plot(model.gam$finalModel, select = 8)
plot(model.gam$finalModel, select = 9)
plot(model.gam$finalModel, select = 11)
plot(model.gam$finalModel, select = 13)
plot(model.gam$finalModel, select = 14)
plot(model.gam$finalModel, select = 15)
plot(model.gam$finalModel, select = 16)
```

For s(f_undergrad, 5.94), s(accept, 4.26), and s(apps, 5.04), we can see that the variances of their effects tend to increase as the corresponding predictor values increase, relative to the other covariates in the model. 

```{r}
# Obtain the test error
gam.pred <- predict(model.gam, newdata = test)
mean((gam.pred - pull(test, "outstate"))^2) # test error
```

The test error is $3.83\times10^6$

# (d) MARS vs linear model
```{r}
# fit a linear model 
set.seed(100)
model.lm <- train(x = train[2:17],
                   y = train$outstate,
                   method = "lm",
                   metric = "RMSE",
                   trControl = ctrl)

# check the model
summary(model.lm$finalModel)
```

I used 10-fold cross validation to train the linear model including all the predictors. Now, let's compare the RMSEs between the two models using the resampling method.

```{r}
# resampling
resamp <- resamples(
  list(
    MARS = model.mars,
    LM = model.lm
  )
)

summary(resamp)

# visualize RMSEs
bwplot(resamp, metric = "RMSE")
```

Both the plot and summary output suggest that the MARS model outperforms the linear model in predicting out-of-state tuition. Therefore, in this case, I would prefer the MARS model.     
In general applications, I think the superiority of a MARS model over a linear model depends on various factors. A MARS model may outperform a linear model when the data exhibits nonlinear relationships. However, if the data suggests linearity and parsimony is preferred for interpretability and computational efficiency, then a linear model may be favored.

