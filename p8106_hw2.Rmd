---
title: "Homework 2"
author: "Yuki Joyama"
date: "2024-03-11"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message=FALSE,
  warning=FALSE
  )
```

```{r library}
# load libraries
library(tidyverse)
library(rsample) # split data
library(caret)
library(splines)
library(mgcv)
library(earth)
library(ggplot2)

# library(tidymodels)
# library(plotmo)
# library(kknn)
# library(FNN) 
# library(pls)
```

```{r dataprep}
# read csv files 
df = read_csv("./College.csv") |> 
  janitor::clean_names()

# partition (training:test=80:20)
set.seed(100)
data_split = initial_split(df, prop = .80)
train = training(data_split)
test = testing(data_split)
```

The college data is split into train (80%) and test (20%).

# a
```{r}
# Function to fit smoothing spline model and return predicted values
fit_spline_model <- function(df, df_value) {
  fit.ss <- smooth.spline(df$perc_alumni, y = df$outstate, df = df_value)
  pred.ss <- predict(fit.ss, x = df$perc_alumni)
  return(data.frame(pred = pred.ss$y, perc = df$perc_alumni))
}

# Function to plot smoothed lines with different colors
plot_smooth_lines <- function(train, df_values, colors) {
  p <- ggplot(data = train, aes(x = perc_alumni, y = outstate)) +
    geom_point(color = rgb(.2, .4, .2, .5))
  
  for (i in seq_along(df_values)) {
    df_value <- df_values[i]
    color <- colors[i]
    
    pred.ss.df <- fit_spline_model(train, df_value)
    
    p <- p + geom_line(aes(x = perc, y = pred), data = pred.ss.df, color = color)
  }
  
  p <- p + theme_bw() 
  return(p)
}

# Set range of dfs 
df_values <- c(seq(2, 30, by = 2))
colors <- rainbow(length(df_values))

# Plot smoothed lines
plot_smooth_lines(train, df_values, colors)
```

I set the range of degree of freedom (df) from 2 to 30 by 2 (2, 4, 6, ..., 28, 30). The plot shows that as df increases, the fitted lines become more wiggly.   

To find the optimal df for the model, I will use Generalized cross-validation.  

```{r}
# refit the model using GCV
fit.ss <- smooth.spline(train$perc_alumni, y = train$outstate, cv = FALSE) # determine tuning parameter by min GCV

pred.ss <- predict(
  fit.ss,
  x = train$perc_alumni
)

pred.ss.df <- data.frame(
  pred = pred.ss$y,
  perc = train$perc_alumni
)

# plot
p <- ggplot(
  data = train,
  aes(x = perc_alumni, y = outstate)
) +
  geom_point(color = rgb(.2, .4, .2, .5))

p + geom_line(
  aes(x = perc, y = pred), 
  data = pred.ss.df,
  color = "red"
) + theme_bw()

```

The selected df was `r round(fit.ss$df, 2)` and the plot of this optimal fit is shown above. 

# b
```{r cv}
# set up 10-fold cross validation 
ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE
)
```

```{r}
set.seed(100)

# model.mars <- train(
#   x = train[, "perc_alumni"],
#   y = train$outstate,
#   data = train,
#   method = "earth",
#   tuneGrid = expand.grid(degree = 1:20, nprune = 2:20),
#   metric = "ROC",
#   trControl = ctrl
# )
```





